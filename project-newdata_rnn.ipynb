{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from time import time\n",
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from keras.optimizers import SGD, Adam, Nadam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dropout, Input, Dense,Lambda, BatchNormalization\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Model,Sequential, load_model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, LSTM, concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Metricas ##########\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.exp(K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)))\n",
    "\n",
    "def coeff_determination_simple(y_true, y_pred):\n",
    "    SS_res =  np.sum(np.square( y_true - y_pred )) \n",
    "    SS_tot = np.sum(np.square( y_true - np.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "###################### New MEthods ##########\n",
    "def load_csv_data(url, drop_columns=[]):\n",
    "    \n",
    "    #READ data\n",
    "    print(\"Loading data\")\n",
    "    df = pd.read_csv(url)\n",
    "    print(\"Removing columns\",drop_columns)\n",
    "    df = df.drop(columns=drop_columns)\n",
    "    return df\n",
    "\n",
    "def log_targets(y_train, y_val, y_test):\n",
    "    \"\"\" Transform to log scale \"\"\"\n",
    "    y_train_log = np.log(y_train)\n",
    "    y_val_log = np.log(y_val)\n",
    "    y_test_log = np.log(y_test)\n",
    "\n",
    "    y_train_log_min = np.min(y_train_log)\n",
    "    y_train_min = np.min(y_train)\n",
    "\n",
    "    y_train_log_max = np.max(y_train_log)\n",
    "    y_train_max = np.max(y_train)\n",
    "\n",
    "    print(\"targets min:{} max: {}\".format(y_train_min, y_train_max))\n",
    "    print(\"targets in log scale min:{} max: {}\".format(y_train_log_min, y_train_log_max))\n",
    "    return y_train_log, y_val_log, y_test_log\n",
    "\n",
    "def normalize_target(y_train_log, y_val_log, y_test_log):\n",
    "    \"\"\"\n",
    "    Normalize data using StandardScaler.\n",
    "    \n",
    "    return scaler object; values of train,val and test sets standarized. \n",
    "    \"\"\"\n",
    "    #Standarización del target\n",
    "    scaler = StandardScaler()\n",
    "    y_train_log_std = scaler.fit_transform(y_train_log)\n",
    "    y_val_log_std = scaler.transform(y_val_log)\n",
    "    y_test_log_std = scaler.transform(y_test_log)\n",
    "    return scaler, y_train_log_std, y_val_log_std, y_test_log_std\n",
    "\n",
    "def get_metrics(model, scaler, x_data, y_true_data, label_set=\"Data\"):\n",
    "    y_pred = np.exp(scaler.inverse_transform(model.predict(x_data).reshape(-1, 1)))\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_data, y_pred))\n",
    "    r2 = r2_score(y_true_data, y_pred)\n",
    "    print(\"RMSE \"+label_set, rmse)\n",
    "    print(\"R2 SCORE \"+label_set, r2)\n",
    "    return rmse, r2\n",
    "\n",
    "def plot_history(history, metrics_list, start_at_epoch=0):\n",
    "    for metric in metrics_list:\n",
    "        plt.plot(history.history[metric][start_at_epoch:],label=metric)\n",
    "    plt.legend()\n",
    "    plt.title(\"Metrics by epochs(Start from epoch:{})\".format(start_at_epoch))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove columns with same values: Index(['path*', 'pathN*', 'path+', 'pathN+', 'notoneof', 'tolist', 'multi',\n",
      "       'top', 'assign', 'sequence'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['triple', 'bgp', 'join', 'leftjoin', 'union', 'filter', 'graph',\n",
       "       'extend', 'minus', 'order', 'project', 'distinct', 'reduced', 'group',\n",
       "       'slice', 'treesize', 'id', 'time', 'pcs0', 'pcs1', 'pcs2', 'pcs3',\n",
       "       'pcs4', 'pcs5', 'pcs6', 'pcs7', 'pcs8', 'pcs9', 'pcs10', 'pcs11',\n",
       "       'pcs12', 'pcs13', 'pcs14', 'pcs15', 'pcs16', 'pcs17', 'pcs18', 'pcs19',\n",
       "       'pcs20', 'pcs21', 'pcs22', 'pcs23', 'pcs24'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load new data\n",
    "data_rnn =  pd.read_csv(\"models_newdata/datasetlsq_output_recurrentfeat.csv\", delimiter=\"ᶶ\")\n",
    "data_rnn = data_rnn.drop(columns=\"Unnamed: 3\")\n",
    "\n",
    "#Data from files\n",
    "data_algebra = pd.read_csv(\"/home/dcasals/graph-edit-distance/data/algebra_features.txt\")\n",
    "data_medoids = pd.read_csv(\"/home/dcasals/graph-edit-distance/data/vectors_medoids.csv\")\n",
    "result = data_algebra.merge(data_medoids, left_on='query_id', right_on='id')\n",
    "result.head()\n",
    "result = result.drop(columns=[\"query_id\",\"Unnamed: 27_x\", \"Unnamed: 27_y\"])\n",
    "result.columns\n",
    "\n",
    "nunique = result.apply(pd.Series.nunique)\n",
    "cols_to_drop = nunique[nunique == 1].index\n",
    "print(\"Remove columns with same values: {}\".format(cols_to_drop))\n",
    "result = result.drop(cols_to_drop, axis=1)\n",
    "new_cols = list(result.columns[:-25]) + ['pcs'+str(i) for i in list(range(0,25))]\n",
    "result.columns = new_cols\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>triple</th>\n",
       "      <th>bgp</th>\n",
       "      <th>join</th>\n",
       "      <th>leftjoin</th>\n",
       "      <th>union</th>\n",
       "      <th>filter</th>\n",
       "      <th>graph</th>\n",
       "      <th>extend</th>\n",
       "      <th>minus</th>\n",
       "      <th>order</th>\n",
       "      <th>...</th>\n",
       "      <th>pcs15</th>\n",
       "      <th>pcs16</th>\n",
       "      <th>pcs17</th>\n",
       "      <th>pcs18</th>\n",
       "      <th>pcs19</th>\n",
       "      <th>pcs20</th>\n",
       "      <th>pcs21</th>\n",
       "      <th>pcs22</th>\n",
       "      <th>pcs23</th>\n",
       "      <th>pcs24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19108</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19109</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19110</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19111</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19112</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19113 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       triple  bgp  join  leftjoin  union  filter  graph  extend  minus  \\\n",
       "0         2.0  1.0   0.0       0.0    0.0     0.0    0.0     0.0    0.0   \n",
       "1         1.0  1.0   0.0       0.0    0.0     0.0    0.0     0.0    0.0   \n",
       "2         1.0  1.0   0.0       0.0    0.0     0.0    0.0     0.0    0.0   \n",
       "3         3.0  2.0   0.0       1.0    0.0     0.0    0.0     0.0    0.0   \n",
       "4         1.0  1.0   0.0       0.0    0.0     0.0    0.0     0.0    0.0   \n",
       "...       ...  ...   ...       ...    ...     ...    ...     ...    ...   \n",
       "19108     1.0  1.0   0.0       0.0    0.0     0.0    0.0     0.0    0.0   \n",
       "19109     1.0  1.0   0.0       0.0    0.0     0.0    0.0     0.0    0.0   \n",
       "19110     6.0  6.0   0.0       3.0    2.0     0.0    0.0     0.0    0.0   \n",
       "19111     6.0  6.0   0.0       3.0    2.0     0.0    0.0     0.0    0.0   \n",
       "19112     6.0  6.0   0.0       3.0    2.0     0.0    0.0     0.0    0.0   \n",
       "\n",
       "       order  ...     pcs15     pcs16     pcs17     pcs18     pcs19     pcs20  \\\n",
       "0        0.0  ...  0.400000  0.166667  0.333333  0.500000  0.400000  0.285714   \n",
       "1        0.0  ...  0.666667  0.133333  0.222222  0.400000  0.666667  0.222222   \n",
       "2        0.0  ...  0.666667  0.133333  0.222222  0.400000  0.666667  0.222222   \n",
       "3        0.0  ...  0.333333  0.200000  0.333333  0.333333  0.285714  0.400000   \n",
       "4        0.0  ...  0.666667  0.142857  0.250000  0.400000  0.666667  0.250000   \n",
       "...      ...  ...       ...       ...       ...       ...       ...       ...   \n",
       "19108    1.0  ...  0.666667  0.142857  0.250000  0.400000  0.666667  0.222222   \n",
       "19109    0.0  ...  0.666667  0.133333  0.222222  0.400000  0.666667  0.222222   \n",
       "19110    0.0  ...  0.153846  0.333333  0.333333  0.166667  0.153846  0.250000   \n",
       "19111    0.0  ...  0.153846  0.333333  0.333333  0.166667  0.153846  0.250000   \n",
       "19112    0.0  ...  0.153846  0.333333  0.333333  0.166667  0.153846  0.250000   \n",
       "\n",
       "          pcs21     pcs22     pcs23     pcs24  \n",
       "0      0.200000  0.333333  0.200000  0.200000  \n",
       "1      0.166667  0.222222  0.153846  0.153846  \n",
       "2      0.166667  0.222222  0.153846  0.153846  \n",
       "3      0.200000  0.333333  0.200000  0.200000  \n",
       "4      0.181818  0.250000  0.166667  0.166667  \n",
       "...         ...       ...       ...       ...  \n",
       "19108  0.166667  0.250000  0.166667  0.166667  \n",
       "19109  0.166667  0.222222  0.153846  0.153846  \n",
       "19110  0.200000  0.333333  1.000000  1.000000  \n",
       "19111  0.200000  0.333333  1.000000  1.000000  \n",
       "19112  0.200000  0.333333  1.000000  1.000000  \n",
       "\n",
       "[19113 rows x 43 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['id','triple', 'bgp', 'join', 'leftjoin', 'union', 'filter', 'project',\n",
    "       'distinct', 'treesize', 'time','pcs0', 'pcs1', 'pcs2', 'pcs3', 'pcs4', 'pcs5',\n",
    "       'pcs6', 'pcs7', 'pcs8', 'pcs9', 'pcs10', 'pcs11', 'pcs12', 'pcs13',\n",
    "       'pcs14', 'pcs15', 'pcs16', 'pcs17', 'pcs18', 'pcs19', 'pcs20', 'pcs21',\n",
    "       'pcs22', 'pcs23', 'pcs24']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tpfs</th>\n",
       "      <th>execTime</th>\n",
       "      <th>triple</th>\n",
       "      <th>bgp</th>\n",
       "      <th>join</th>\n",
       "      <th>leftjoin</th>\n",
       "      <th>union</th>\n",
       "      <th>filter</th>\n",
       "      <th>project</th>\n",
       "      <th>...</th>\n",
       "      <th>pcs15</th>\n",
       "      <th>pcs16</th>\n",
       "      <th>pcs17</th>\n",
       "      <th>pcs18</th>\n",
       "      <th>pcs19</th>\n",
       "      <th>pcs20</th>\n",
       "      <th>pcs21</th>\n",
       "      <th>pcs22</th>\n",
       "      <th>pcs23</th>\n",
       "      <th>pcs24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://lsq.aksw.org/res/DBpedia-q113812</td>\n",
       "      <td>2,http://www.w3.org/2000/01/rdf-schema#label,1...</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://lsq.aksw.org/res/DBpedia-q113813</td>\n",
       "      <td>9,http://dbpedia.org/property/pushpinMap,</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://lsq.aksw.org/res/DBpedia-q113814</td>\n",
       "      <td>9,http://dbpedia.org/property/reference,</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://lsq.aksw.org/res/DBpedia-q113815</td>\n",
       "      <td>9,http://www.w3.org/2000/01/rdf-schema#label,9...</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://lsq.aksw.org/res/DBpedia-q113817</td>\n",
       "      <td>9,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16956</th>\n",
       "      <td>http://lsq.aksw.org/res/DBpedia-q1014063</td>\n",
       "      <td>1,http://www.w3.org/2000/01/rdf-schema#label,</td>\n",
       "      <td>55527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16957</th>\n",
       "      <td>http://lsq.aksw.org/res/DBpedia-q1085454</td>\n",
       "      <td>1,http://dbpedia.org/property/label,</td>\n",
       "      <td>16592</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16958</th>\n",
       "      <td>http://lsq.aksw.org/res/LGD-q5773</td>\n",
       "      <td>4,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...</td>\n",
       "      <td>3434</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16959</th>\n",
       "      <td>http://lsq.aksw.org/res/LGD-q5960</td>\n",
       "      <td>4,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...</td>\n",
       "      <td>3140</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16960</th>\n",
       "      <td>http://lsq.aksw.org/res/LGD-q6094</td>\n",
       "      <td>4,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...</td>\n",
       "      <td>2882</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16961 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             id  \\\n",
       "0       http://lsq.aksw.org/res/DBpedia-q113812   \n",
       "1       http://lsq.aksw.org/res/DBpedia-q113813   \n",
       "2       http://lsq.aksw.org/res/DBpedia-q113814   \n",
       "3       http://lsq.aksw.org/res/DBpedia-q113815   \n",
       "4       http://lsq.aksw.org/res/DBpedia-q113817   \n",
       "...                                         ...   \n",
       "16956  http://lsq.aksw.org/res/DBpedia-q1014063   \n",
       "16957  http://lsq.aksw.org/res/DBpedia-q1085454   \n",
       "16958         http://lsq.aksw.org/res/LGD-q5773   \n",
       "16959         http://lsq.aksw.org/res/LGD-q5960   \n",
       "16960         http://lsq.aksw.org/res/LGD-q6094   \n",
       "\n",
       "                                                    tpfs  execTime  triple  \\\n",
       "0      2,http://www.w3.org/2000/01/rdf-schema#label,1...         4     2.0   \n",
       "1              9,http://dbpedia.org/property/pushpinMap,         2     1.0   \n",
       "2               9,http://dbpedia.org/property/reference,         2     1.0   \n",
       "3      9,http://www.w3.org/2000/01/rdf-schema#label,9...         4     3.0   \n",
       "4      9,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...         2     1.0   \n",
       "...                                                  ...       ...     ...   \n",
       "16956      1,http://www.w3.org/2000/01/rdf-schema#label,     55527     1.0   \n",
       "16957               1,http://dbpedia.org/property/label,     16592     1.0   \n",
       "16958  4,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...      3434     6.0   \n",
       "16959  4,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...      3140     6.0   \n",
       "16960  4,http://www.w3.org/1999/02/22-rdf-syntax-ns#t...      2882     6.0   \n",
       "\n",
       "       bgp  join  leftjoin  union  filter  project  ...     pcs15     pcs16  \\\n",
       "0      1.0   0.0       0.0    0.0     0.0      0.0  ...  0.400000  0.166667   \n",
       "1      1.0   0.0       0.0    0.0     0.0      1.0  ...  0.666667  0.133333   \n",
       "2      1.0   0.0       0.0    0.0     0.0      1.0  ...  0.666667  0.133333   \n",
       "3      2.0   0.0       1.0    0.0     0.0      1.0  ...  0.333333  0.200000   \n",
       "4      1.0   0.0       0.0    0.0     0.0      1.0  ...  0.666667  0.142857   \n",
       "...    ...   ...       ...    ...     ...      ...  ...       ...       ...   \n",
       "16956  1.0   0.0       0.0    0.0     0.0      1.0  ...  0.666667  0.142857   \n",
       "16957  1.0   0.0       0.0    0.0     0.0      1.0  ...  0.666667  0.133333   \n",
       "16958  6.0   0.0       3.0    2.0     0.0      1.0  ...  0.153846  0.333333   \n",
       "16959  6.0   0.0       3.0    2.0     0.0      1.0  ...  0.153846  0.333333   \n",
       "16960  6.0   0.0       3.0    2.0     0.0      1.0  ...  0.153846  0.333333   \n",
       "\n",
       "          pcs17     pcs18     pcs19     pcs20     pcs21     pcs22     pcs23  \\\n",
       "0      0.333333  0.500000  0.400000  0.285714  0.200000  0.333333  0.200000   \n",
       "1      0.222222  0.400000  0.666667  0.222222  0.166667  0.222222  0.153846   \n",
       "2      0.222222  0.400000  0.666667  0.222222  0.166667  0.222222  0.153846   \n",
       "3      0.333333  0.333333  0.285714  0.400000  0.200000  0.333333  0.200000   \n",
       "4      0.250000  0.400000  0.666667  0.250000  0.181818  0.250000  0.166667   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "16956  0.250000  0.400000  0.666667  0.222222  0.166667  0.250000  0.166667   \n",
       "16957  0.222222  0.400000  0.666667  0.222222  0.166667  0.222222  0.153846   \n",
       "16958  0.333333  0.166667  0.153846  0.250000  0.200000  0.333333  1.000000   \n",
       "16959  0.333333  0.166667  0.153846  0.250000  0.200000  0.333333  1.000000   \n",
       "16960  0.333333  0.166667  0.153846  0.250000  0.200000  0.333333  1.000000   \n",
       "\n",
       "          pcs24  \n",
       "0      0.200000  \n",
       "1      0.153846  \n",
       "2      0.153846  \n",
       "3      0.200000  \n",
       "4      0.166667  \n",
       "...         ...  \n",
       "16956  0.166667  \n",
       "16957  0.153846  \n",
       "16958  1.000000  \n",
       "16959  1.000000  \n",
       "16960  1.000000  \n",
       "\n",
       "[16961 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rnn = data_rnn.merge(result, left_on='id', right_on='id')\n",
    "data_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tpfs', 'execTime', 'triple', 'bgp', 'join', 'leftjoin', 'union',\n",
       "       'filter', 'project', 'distinct', 'treesize', 'time', 'pcs0', 'pcs1',\n",
       "       'pcs2', 'pcs3', 'pcs4', 'pcs5', 'pcs6', 'pcs7', 'pcs8', 'pcs9', 'pcs10',\n",
       "       'pcs11', 'pcs12', 'pcs13', 'pcs14', 'pcs15', 'pcs16', 'pcs17', 'pcs18',\n",
       "       'pcs19', 'pcs20', 'pcs21', 'pcs22', 'pcs23', 'pcs24'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rnn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "def procesar_tpfs_odd(x):\n",
    "    \"\"\"Get odd elements from a list \"\"\"\n",
    "    lista = []\n",
    "    for a in x.split(\",\")[::2]:\n",
    "        if a != \"\":\n",
    "            if a.isdigit():\n",
    "                lista.append(int(a))\n",
    "            else:\n",
    "                lista.append(0)\n",
    "#     print(lista)\n",
    "    return lista\n",
    "def procesar_tpfs_even(x):\n",
    "    \"\"\"Get even elements from a list \"\"\"\n",
    "    lista  = [a for a in x.split(\",\")[1::2] if a != \"\"]\n",
    "#     print(lista)\n",
    "    return lista\n",
    "def uri_2_index_seq(x,uri2Index):\n",
    "    \"\"\"transform uris to sequences\"\"\"\n",
    "    lista  = [uri2Index[a] for a in x]\n",
    "#     print(lista)\n",
    "    return lista\n",
    "def get_embedding_matrix_zero(index_dict, EMBEDDING_DIM):\n",
    "    '''\n",
    "        Generamos la matriz de embeddings, de dimensiones: \n",
    "          - (Tamano_vocabulario + 1) x Dimesion_embedding DecodeInput\n",
    "    '''\n",
    "    embedding_matrix = np.zeros((len(index_dict) + 1, EMBEDDING_DIM))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_RNN(num_pred_words, embedd_dim, embedd_type_dim, embedd_input_matrix,embedding_types_matrix, lstm_units, lstm_types_units):\n",
    "    dropout = 0.25\n",
    "    lstm_pred_input = Input(shape=(None,),name='lstm_pred_input')\n",
    "    lstm_pred_embedd = Embedding(num_pred_words + 1 ,embedd_dim, weights=[embedd_input_matrix], trainable=True)(lstm_pred_input)\n",
    "    lstm_pred_out = LSTM(lstm_units, name='lstm_pred_out')(lstm_pred_embedd)\n",
    "\n",
    "    # Salida Auxiliar del modelo recurrente de predicados\n",
    "#     aux_recurrent_pred_output = Dense(1, activation='linear', name='aux_recurrent_pred_output')(lstm_pred_out)\n",
    "    \n",
    "    lstm_type_input = Input(shape=(None,),name='lstm_type_input')\n",
    "    lstm_type_embedd = Embedding(10 , embedd_type_dim, weights=[embedding_types_matrix], trainable=True)(lstm_type_input)\n",
    "    lstm_type_out = LSTM(lstm_units, name='lstm_type_out')(lstm_type_embedd)\n",
    "\n",
    "#     Salida Auxiliar del modelo recurrente de predicados\n",
    "#     aux_recurrent_type_output = Dense(1, activation='linear', name='aux_recurrent_type_output')(lstm_type_out)\n",
    "    \n",
    "    dense_input = Input(shape=(34,), name='dense_input')    \n",
    "\n",
    "    # We stack a deep densely-connected network on top\n",
    "    x = Dense(260, activation='relu')(dense_input)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(240, activation='relu')(x)\n",
    "#     x = concatenate([lstm_type_out,lstm_pred_out, x])\n",
    "    x = concatenate([lstm_type_out, x])\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(1, activation='linear', name='main_output')(x)\n",
    "    \n",
    "    #Build model\n",
    "#     model = Model(inputs=[lstm_type_input, lstm_pred_input, dense_input,], outputs=[main_output])\n",
    "    model = Model(inputs=[lstm_type_input, dense_input,], outputs=[main_output])\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file='model.png')\n",
    "    return model\n",
    "\n",
    "def build_RNN2(num_pred_words, embedd_dim, embedd_type_dim, embedd_input_matrix,embedding_types_matrix, lstm_units, lstm_types_units):\n",
    "    dropout = 0.25\n",
    "    lstm_pred_input = Input(shape=(None,),name='lstm_pred_input')\n",
    "    lstm_pred_embedd = Embedding(num_pred_words + 1 ,embedd_dim, weights=[embedd_input_matrix], trainable=True)(lstm_pred_input)\n",
    "    lstm_pred_out = LSTM(lstm_units, name='lstm_pred_out')(lstm_pred_embedd)\n",
    "\n",
    "    # Salida Auxiliar del modelo recurrente de predicados\n",
    "#     aux_recurrent_pred_output = Dense(1, activation='linear', name='aux_recurrent_pred_output')(lstm_pred_out)\n",
    "    \n",
    "    lstm_type_input = Input(shape=(None,),name='lstm_type_input')\n",
    "    lstm_type_embedd = Embedding(10 , embedd_type_dim, weights=[embedding_types_matrix], trainable=True)(lstm_type_input)\n",
    "    lstm_type_out = LSTM(lstm_units, name='lstm_type_out')(lstm_type_embedd)\n",
    "\n",
    "#     Salida Auxiliar del modelo recurrente de predicados\n",
    "#     aux_recurrent_type_output = Dense(1, activation='linear', name='aux_recurrent_type_output')(lstm_type_out)\n",
    "    \n",
    "    dense_input = Input(shape=(34,), name='dense_input')    \n",
    "\n",
    "    # We stack a deep densely-connected network on top\n",
    "    x = Dense(260, activation='relu')(dense_input)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(240, activation='relu')(x)\n",
    "    x = concatenate([lstm_type_out, lstm_pred_out])\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(1, activation='linear', name='main_output')(x)\n",
    "    \n",
    "    #Build model\n",
    "    model = Model(inputs=[lstm_type_input, lstm_pred_input,], outputs=[main_output])\n",
    "#     model = Model(inputs=[lstm_type_input, dense_input,], outputs=[main_output])\n",
    "    return model\n",
    "\n",
    "def build_ANN2():\n",
    "    dropout = 0.25\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(260, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(300, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(240, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "#     model = Model(inputs=[lstm_type_input, dense_input,], outputs=[main_output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_data(data_rnn):\n",
    "    tpfs_structure = data_rnn['tpfs'].apply(lambda x: procesar_tpfs_odd(x))\n",
    "    tpfs_predicate = data_rnn['tpfs'].apply(lambda x: procesar_tpfs_even(x))\n",
    "    \n",
    "    dense_data = data_rnn[['triple', 'bgp', 'join', 'leftjoin', 'union', 'filter', 'project',\n",
    "       'distinct', 'treesize','pcs0', 'pcs1', 'pcs2', 'pcs3', 'pcs4', 'pcs5',\n",
    "       'pcs6', 'pcs7', 'pcs8', 'pcs9', 'pcs10', 'pcs11', 'pcs12', 'pcs13',\n",
    "       'pcs14', 'pcs15', 'pcs16', 'pcs17', 'pcs18', 'pcs19', 'pcs20', 'pcs21',\n",
    "       'pcs22', 'pcs23', 'pcs24']]\n",
    "    targets = data_rnn[['time']]\n",
    "    print(dense_data.columns)\n",
    "    return dense_data.values, tpfs_structure, tpfs_predicate, targets.values\n",
    "\n",
    "def preprocesar_lstm_data(lstm_pred_data, lstm_type_data, EMBEDDING_DIM_PRED = 200, EMBEDDING_DIM_TYPES=10):\n",
    "    #create indexes\n",
    "    index = 1\n",
    "    uri2Index = {}\n",
    "    index2Uri = {}\n",
    "\n",
    "    for lista in  lstm_pred_data.values:\n",
    "        for val in lista:\n",
    "            if val not in uri2Index:\n",
    "                uri2Index[val] = index\n",
    "                index2Uri[index] = val\n",
    "                index +=1\n",
    "    tpfs_pred_index = lstm_pred_data.apply(lambda x: uri_2_index_seq(x, uri2Index))\n",
    "    \n",
    "    #Max Len\n",
    "    max_length = np.max(tpfs_pred_index.apply(lambda x: len(x)))\n",
    "    paded_tpf_pred_data = np.array(pad_sequences(tpfs_pred_index.values.tolist(), maxlen=15, padding='post'))\n",
    "    paded_tpf_types_data = np.array(pad_sequences(lstm_type_data.values.tolist(), maxlen=15, padding='post'))\n",
    "    \n",
    "    #Embedding Matrix\n",
    "    \n",
    "    emb_matrix_pred = get_embedding_matrix_zero(uri2Index, EMBEDDING_DIM_PRED)\n",
    "    emb_matrix_types = np.zeros((9 + 1, EMBEDDING_DIM_TYPES))\n",
    "    num_pred_words = len(uri2Index)\n",
    "    \n",
    "    return paded_tpf_pred_data, paded_tpf_types_data, num_pred_words, emb_matrix_pred, emb_matrix_types, uri2Index, index2Uri\n",
    "\n",
    "def split_by_index(data, train_indexes, val_indexes, test_indexes):\n",
    "    x_train      = data[train_indexes]\n",
    "    x_val = data[val_indexes]\n",
    "    x_test = data[test_indexes]\n",
    "    return x_train, x_val, x_test\n",
    "    \n",
    "# lstm_type],scalery, y_train_log_std, y_val_log_std, y_test_log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['triple', 'bgp', 'join', 'leftjoin', 'union', 'filter', 'project',\n",
      "       'distinct', 'treesize', 'pcs0', 'pcs1', 'pcs2', 'pcs3', 'pcs4', 'pcs5',\n",
      "       'pcs6', 'pcs7', 'pcs8', 'pcs9', 'pcs10', 'pcs11', 'pcs12', 'pcs13',\n",
      "       'pcs14', 'pcs15', 'pcs16', 'pcs17', 'pcs18', 'pcs19', 'pcs20', 'pcs21',\n",
      "       'pcs22', 'pcs23', 'pcs24'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dense_data, lstm_type_data, lstm_pred_data,  targets = separar_data(data_rnn)\n",
    "EMBEDDING_DIM_PRED = 100\n",
    "EMBEDDING_DIM_TYPES = 10\n",
    "pad_pred_data, pad_types_data, num_pred_words, emb_matrix_pred, emb_matrix_types, uri2Index, index2Uri = preprocesar_lstm_data(lstm_pred_data, lstm_type_data, EMBEDDING_DIM_PRED, EMBEDDING_DIM_TYPES)\n",
    "\n",
    "#We split the train indexes, and val indexes to half for get test_data\n",
    "all_indexes = list(range(dense_data.shape[0]))\n",
    "train_indexes, temp_indexes = train_test_split(all_indexes, test_size=0.3)\n",
    "test_indexes, val_indexes = train_test_split(temp_indexes, test_size=0.5)\n",
    "\n",
    "x_train, x_val, x_test = split_by_index(dense_data, train_indexes, val_indexes, test_indexes)\n",
    "y_train, y_val, y_test = split_by_index(targets, train_indexes, val_indexes, test_indexes)\n",
    "\n",
    "x_train_lstm_pred, x_val_lstm_pred, x_test_lstm_pred  = split_by_index(pad_pred_data, train_indexes, val_indexes, test_indexes)\n",
    "x_train_lstm_type, x_val_lstm_type, x_test_lstm_type = split_by_index(pad_types_data, train_indexes, val_indexes, test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets min:1 max: 118554\n",
      "targets in log scale min:0.0 max: 11.683123831961872\n",
      "Shapes Train (11872, 34) (11872, 15) (11872, 15)\n",
      "Shapes VAL (2545, 34) (2545, 15) (2545, 15)\n",
      "Shapes TEST (2544, 34) (2544, 15) (2544, 15)\n"
     ]
    }
   ],
   "source": [
    "#Log and Normalize\n",
    "y_train_log, y_val_log, y_test_log = log_targets(y_train, y_val, y_test)\n",
    "scalery, y_train_log_std, y_val_log_std, y_test_log_std = normalize_target(y_train_log, y_val_log, y_test_log)\n",
    "\n",
    "print(\"Shapes Train\", x_train.shape,x_train_lstm_pred.shape, x_train_lstm_type.shape)\n",
    "print(\"Shapes VAL\", x_val.shape, x_val_lstm_pred.shape, x_val_lstm_type.shape)\n",
    "print(\"Shapes TEST\", x_test.shape, x_test_lstm_pred.shape, x_test_lstm_type.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_ann(model, x_train, y_train, x_val, y_val,optimizer=None):\n",
    "    callbacks_best = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "                          ModelCheckpoint(filepath='models_newdata/bestm_mixed.hdf5',\n",
    "                                          monitor='val_loss', save_best_only=True\n",
    "                                         )]\n",
    "    if optimizer is None:\n",
    "        optimizer=Adam(learning_rate=0.00015)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[coeff_determination, rmse])\n",
    "    t0=time()\n",
    "    epochs = 550\n",
    "    print(\"before train: Init time: {}\".format(round(t0,3)))\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=120,\n",
    "                    validation_data=( x_val, y_val),\n",
    "                    callbacks=callbacks_best,\n",
    "                    verbose=True\n",
    "                     )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM_PRED = 100\n",
    "EMBEDDING_DIM_TYPES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = build_RNN(\n",
    "    num_pred_words,\n",
    "    EMBEDDING_DIM_PRED,\n",
    "    EMBEDDING_DIM_TYPES,\n",
    "    emb_matrix_pred,\n",
    "    emb_matrix_types,\n",
    "    lstm_units=32,\n",
    "    lstm_types_units=5\n",
    "    )\n",
    "model,history = train_rnn_ann(model, x_train_lstm_type, x_train_lstm_pred, x_train, y_train_log_std,x_val_lstm_type, x_val_lstm_pred, x_val, y_val_log_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = build_RNN2(\n",
    "    num_pred_words,\n",
    "    EMBEDDING_DIM_PRED,\n",
    "    EMBEDDING_DIM_TYPES,\n",
    "    emb_matrix_pred,\n",
    "    emb_matrix_types,\n",
    "    lstm_units=32,\n",
    "    lstm_types_units=5\n",
    "    )\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "model,history = train_rnn_ann(model, [x_train_lstm_type, x_train_lstm_pred], y_train_log_std,[x_val_lstm_type, x_val_lstm_pred], y_val_log_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = build_ANN2(\n",
    "        num_pred_words,\n",
    "        EMBEDDING_DIM_PRED,\n",
    "        EMBEDDING_DIM_TYPES,\n",
    "        emb_matrix_pred,\n",
    "        emb_matrix_types,\n",
    "        lstm_units=32,\n",
    "        lstm_types_units=5\n",
    "    )\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "model,history = train_rnn_ann(model, [x_train], y_train_log_std,[x_val], y_val_log_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_ann(x_train, y_train, x_val, y_val, n1, n2, n3, epochs, optimizer, dropout, verbose=False):\n",
    "    \n",
    "    callbacks_best = [EarlyStopping(monitor='val_loss', patience=20),\n",
    "                      ModelCheckpoint(filepath='models_newdata/bestm_newdata.h5'.format(n1,n2,n3),\n",
    "                                      monitor='val_loss', save_best_only=True\n",
    "                                     )]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n1, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n2, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(n3, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae', coeff_determination, rmse])\n",
    "    t0=time()\n",
    "    print(\"before train: Init time: {}\".format(round(t0,3)))\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=120,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=callbacks_best,\n",
    "                    verbose=verbose\n",
    "                     )\n",
    "    t1=time()\n",
    "    print(\"after train, finish time: {}\".format(round(t1,3)))\n",
    "    print(\"training time {}\",format(round(t1-t0, 3)))\n",
    "    print(model.summary())\n",
    "\n",
    "    return model, history, round(t1-t0, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before train: Init time: 1585600073.04\n",
      "Train on 11872 samples, validate on 2545 samples\n",
      "Epoch 1/500\n",
      "11872/11872 [==============================] - 1s 104us/step - loss: 0.6868 - mae: 0.7209 - coeff_determination: 0.3069 - rmse: 2.2562 - val_loss: 0.5140 - val_mae: 0.5426 - val_coeff_determination: 0.5015 - val_rmse: 1.9814\n",
      "Epoch 2/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.4924 - mae: 0.5589 - coeff_determination: 0.5043 - rmse: 1.9547 - val_loss: 0.4109 - val_mae: 0.4281 - val_coeff_determination: 0.5997 - val_rmse: 1.8073\n",
      "Epoch 3/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.4137 - mae: 0.4806 - coeff_determination: 0.5843 - rmse: 1.8272 - val_loss: 0.3860 - val_mae: 0.3889 - val_coeff_determination: 0.6237 - val_rmse: 1.7685\n",
      "Epoch 4/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.3658 - mae: 0.4292 - coeff_determination: 0.6326 - rmse: 1.7457 - val_loss: 0.3649 - val_mae: 0.3629 - val_coeff_determination: 0.6436 - val_rmse: 1.7221\n",
      "Epoch 5/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.3448 - mae: 0.4056 - coeff_determination: 0.6538 - rmse: 1.7085 - val_loss: 0.3603 - val_mae: 0.3345 - val_coeff_determination: 0.6487 - val_rmse: 1.7071\n",
      "Epoch 6/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.3327 - mae: 0.3909 - coeff_determination: 0.6654 - rmse: 1.6862 - val_loss: 0.3425 - val_mae: 0.3343 - val_coeff_determination: 0.6652 - val_rmse: 1.6730\n",
      "Epoch 7/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.3207 - mae: 0.3777 - coeff_determination: 0.6769 - rmse: 1.6614 - val_loss: 0.3506 - val_mae: 0.3383 - val_coeff_determination: 0.6567 - val_rmse: 1.6962\n",
      "Epoch 8/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.3151 - mae: 0.3680 - coeff_determination: 0.6832 - rmse: 1.6515 - val_loss: 0.3318 - val_mae: 0.3239 - val_coeff_determination: 0.6752 - val_rmse: 1.6516\n",
      "Epoch 9/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.3064 - mae: 0.3611 - coeff_determination: 0.6923 - rmse: 1.6349 - val_loss: 0.3326 - val_mae: 0.3284 - val_coeff_determination: 0.6745 - val_rmse: 1.6598\n",
      "Epoch 10/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.3021 - mae: 0.3582 - coeff_determination: 0.6962 - rmse: 1.6277 - val_loss: 0.3311 - val_mae: 0.3280 - val_coeff_determination: 0.6757 - val_rmse: 1.6557\n",
      "Epoch 11/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2972 - mae: 0.3495 - coeff_determination: 0.7013 - rmse: 1.6174 - val_loss: 0.3307 - val_mae: 0.3292 - val_coeff_determination: 0.6764 - val_rmse: 1.6578\n",
      "Epoch 12/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2960 - mae: 0.3488 - coeff_determination: 0.7030 - rmse: 1.6144 - val_loss: 0.3308 - val_mae: 0.3303 - val_coeff_determination: 0.6755 - val_rmse: 1.6605\n",
      "Epoch 13/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2924 - mae: 0.3441 - coeff_determination: 0.7062 - rmse: 1.6078 - val_loss: 0.3316 - val_mae: 0.3230 - val_coeff_determination: 0.6747 - val_rmse: 1.6577\n",
      "Epoch 14/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2902 - mae: 0.3411 - coeff_determination: 0.7084 - rmse: 1.6052 - val_loss: 0.3343 - val_mae: 0.3345 - val_coeff_determination: 0.6719 - val_rmse: 1.6683\n",
      "Epoch 15/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2845 - mae: 0.3355 - coeff_determination: 0.7135 - rmse: 1.5919 - val_loss: 0.3336 - val_mae: 0.3456 - val_coeff_determination: 0.6729 - val_rmse: 1.6757\n",
      "Epoch 16/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2837 - mae: 0.3333 - coeff_determination: 0.7144 - rmse: 1.5882 - val_loss: 0.3274 - val_mae: 0.3445 - val_coeff_determination: 0.6793 - val_rmse: 1.6632\n",
      "Epoch 17/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2804 - mae: 0.3295 - coeff_determination: 0.7185 - rmse: 1.5829 - val_loss: 0.3371 - val_mae: 0.3345 - val_coeff_determination: 0.6697 - val_rmse: 1.6786\n",
      "Epoch 18/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2778 - mae: 0.3273 - coeff_determination: 0.7205 - rmse: 1.5788 - val_loss: 0.3240 - val_mae: 0.3567 - val_coeff_determination: 0.6826 - val_rmse: 1.6612\n",
      "Epoch 19/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2794 - mae: 0.3267 - coeff_determination: 0.7191 - rmse: 1.5919 - val_loss: 0.3244 - val_mae: 0.3549 - val_coeff_determination: 0.6818 - val_rmse: 1.6588\n",
      "Epoch 20/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2758 - mae: 0.3246 - coeff_determination: 0.7224 - rmse: 1.5734 - val_loss: 0.3294 - val_mae: 0.3504 - val_coeff_determination: 0.6769 - val_rmse: 1.6720\n",
      "Epoch 21/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2722 - mae: 0.3215 - coeff_determination: 0.7267 - rmse: 1.5683 - val_loss: 0.3318 - val_mae: 0.3394 - val_coeff_determination: 0.6749 - val_rmse: 1.6726\n",
      "Epoch 22/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2728 - mae: 0.3202 - coeff_determination: 0.7257 - rmse: 1.5668 - val_loss: 0.3215 - val_mae: 0.3348 - val_coeff_determination: 0.6848 - val_rmse: 1.6481\n",
      "Epoch 23/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2679 - mae: 0.3145 - coeff_determination: 0.7305 - rmse: 1.5569 - val_loss: 0.3220 - val_mae: 0.3508 - val_coeff_determination: 0.6843 - val_rmse: 1.6575\n",
      "Epoch 24/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2679 - mae: 0.3141 - coeff_determination: 0.7305 - rmse: 1.5564 - val_loss: 0.3278 - val_mae: 0.3598 - val_coeff_determination: 0.6782 - val_rmse: 1.6717\n",
      "Epoch 25/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2675 - mae: 0.3126 - coeff_determination: 0.7315 - rmse: 1.5565 - val_loss: 0.3289 - val_mae: 0.3477 - val_coeff_determination: 0.6769 - val_rmse: 1.6685\n",
      "Epoch 26/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.2645 - mae: 0.3107 - coeff_determination: 0.7345 - rmse: 1.5500 - val_loss: 0.3292 - val_mae: 0.3438 - val_coeff_determination: 0.6775 - val_rmse: 1.6695\n",
      "Epoch 27/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.2645 - mae: 0.3091 - coeff_determination: 0.7349 - rmse: 1.5497 - val_loss: 0.3187 - val_mae: 0.3522 - val_coeff_determination: 0.6883 - val_rmse: 1.6528\n",
      "Epoch 28/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2623 - mae: 0.3073 - coeff_determination: 0.7372 - rmse: 1.5456 - val_loss: 0.3201 - val_mae: 0.3612 - val_coeff_determination: 0.6865 - val_rmse: 1.6570\n",
      "Epoch 29/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2604 - mae: 0.3077 - coeff_determination: 0.7380 - rmse: 1.5415 - val_loss: 0.3178 - val_mae: 0.3572 - val_coeff_determination: 0.6886 - val_rmse: 1.6501\n",
      "Epoch 30/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2599 - mae: 0.3047 - coeff_determination: 0.7383 - rmse: 1.5385 - val_loss: 0.3187 - val_mae: 0.3561 - val_coeff_determination: 0.6880 - val_rmse: 1.6535\n",
      "Epoch 31/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.2594 - mae: 0.3041 - coeff_determination: 0.7396 - rmse: 1.5378 - val_loss: 0.3257 - val_mae: 0.3782 - val_coeff_determination: 0.6807 - val_rmse: 1.6714\n",
      "Epoch 32/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2567 - mae: 0.3029 - coeff_determination: 0.7423 - rmse: 1.5334 - val_loss: 0.3242 - val_mae: 0.3433 - val_coeff_determination: 0.6821 - val_rmse: 1.6539\n",
      "Epoch 33/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2564 - mae: 0.3022 - coeff_determination: 0.7428 - rmse: 1.5338 - val_loss: 0.3236 - val_mae: 0.3476 - val_coeff_determination: 0.6833 - val_rmse: 1.6611\n",
      "Epoch 34/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2545 - mae: 0.2990 - coeff_determination: 0.7441 - rmse: 1.5286 - val_loss: 0.3190 - val_mae: 0.3632 - val_coeff_determination: 0.6873 - val_rmse: 1.6563\n",
      "Epoch 35/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2535 - mae: 0.2998 - coeff_determination: 0.7455 - rmse: 1.5280 - val_loss: 0.3242 - val_mae: 0.3429 - val_coeff_determination: 0.6832 - val_rmse: 1.6619\n",
      "Epoch 36/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2541 - mae: 0.2977 - coeff_determination: 0.7442 - rmse: 1.5275 - val_loss: 0.3137 - val_mae: 0.3661 - val_coeff_determination: 0.6943 - val_rmse: 1.6526\n",
      "Epoch 37/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2549 - mae: 0.3003 - coeff_determination: 0.7430 - rmse: 1.5305 - val_loss: 0.3281 - val_mae: 0.3587 - val_coeff_determination: 0.6788 - val_rmse: 1.6727\n",
      "Epoch 38/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2513 - mae: 0.2961 - coeff_determination: 0.7472 - rmse: 1.5219 - val_loss: 0.3020 - val_mae: 0.3390 - val_coeff_determination: 0.7056 - val_rmse: 1.6218\n",
      "Epoch 39/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2497 - mae: 0.2937 - coeff_determination: 0.7487 - rmse: 1.5193 - val_loss: 0.3065 - val_mae: 0.3276 - val_coeff_determination: 0.7010 - val_rmse: 1.6235\n",
      "Epoch 40/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2484 - mae: 0.2942 - coeff_determination: 0.7503 - rmse: 1.5178 - val_loss: 0.3038 - val_mae: 0.3515 - val_coeff_determination: 0.7031 - val_rmse: 1.6236\n",
      "Epoch 41/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2495 - mae: 0.2947 - coeff_determination: 0.7496 - rmse: 1.5188 - val_loss: 0.3028 - val_mae: 0.3296 - val_coeff_determination: 0.7051 - val_rmse: 1.6223\n",
      "Epoch 42/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2471 - mae: 0.2929 - coeff_determination: 0.7516 - rmse: 1.5147 - val_loss: 0.3091 - val_mae: 0.3482 - val_coeff_determination: 0.6989 - val_rmse: 1.6409\n",
      "Epoch 43/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2453 - mae: 0.2902 - coeff_determination: 0.7535 - rmse: 1.5113 - val_loss: 0.3124 - val_mae: 0.3599 - val_coeff_determination: 0.6947 - val_rmse: 1.6460\n",
      "Epoch 44/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2437 - mae: 0.2906 - coeff_determination: 0.7551 - rmse: 1.5111 - val_loss: 0.3049 - val_mae: 0.3463 - val_coeff_determination: 0.7023 - val_rmse: 1.6272\n",
      "Epoch 45/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2434 - mae: 0.2898 - coeff_determination: 0.7556 - rmse: 1.5088 - val_loss: 0.2975 - val_mae: 0.3336 - val_coeff_determination: 0.7114 - val_rmse: 1.6183\n",
      "Epoch 46/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2394 - mae: 0.2859 - coeff_determination: 0.7598 - rmse: 1.5006 - val_loss: 0.2926 - val_mae: 0.3326 - val_coeff_determination: 0.7168 - val_rmse: 1.6096\n",
      "Epoch 47/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2395 - mae: 0.2861 - coeff_determination: 0.7596 - rmse: 1.5014 - val_loss: 0.2974 - val_mae: 0.3271 - val_coeff_determination: 0.7108 - val_rmse: 1.6113\n",
      "Epoch 48/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.2356 - mae: 0.2828 - coeff_determination: 0.7631 - rmse: 1.4945 - val_loss: 0.2854 - val_mae: 0.3186 - val_coeff_determination: 0.7232 - val_rmse: 1.5881\n",
      "Epoch 49/500\n",
      "11872/11872 [==============================] - 1s 71us/step - loss: 0.2390 - mae: 0.2834 - coeff_determination: 0.7598 - rmse: 1.4997 - val_loss: 0.2892 - val_mae: 0.3396 - val_coeff_determination: 0.7198 - val_rmse: 1.6081\n",
      "Epoch 50/500\n",
      "11872/11872 [==============================] - 1s 67us/step - loss: 0.2347 - mae: 0.2830 - coeff_determination: 0.7640 - rmse: 1.4930 - val_loss: 0.2836 - val_mae: 0.3238 - val_coeff_determination: 0.7259 - val_rmse: 1.5894\n",
      "Epoch 51/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.2343 - mae: 0.2808 - coeff_determination: 0.7644 - rmse: 1.4927 - val_loss: 0.2820 - val_mae: 0.3196 - val_coeff_determination: 0.7268 - val_rmse: 1.5821\n",
      "Epoch 52/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2332 - mae: 0.2807 - coeff_determination: 0.7659 - rmse: 1.4910 - val_loss: 0.2847 - val_mae: 0.3328 - val_coeff_determination: 0.7236 - val_rmse: 1.5933\n",
      "Epoch 53/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2336 - mae: 0.2785 - coeff_determination: 0.7654 - rmse: 1.4895 - val_loss: 0.2825 - val_mae: 0.3216 - val_coeff_determination: 0.7266 - val_rmse: 1.5867\n",
      "Epoch 54/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.2313 - mae: 0.2788 - coeff_determination: 0.7681 - rmse: 1.4869 - val_loss: 0.2822 - val_mae: 0.3162 - val_coeff_determination: 0.7269 - val_rmse: 1.5850\n",
      "Epoch 55/500\n",
      "11872/11872 [==============================] - 1s 72us/step - loss: 0.2291 - mae: 0.2746 - coeff_determination: 0.7698 - rmse: 1.4828 - val_loss: 0.2828 - val_mae: 0.3214 - val_coeff_determination: 0.7264 - val_rmse: 1.5859\n",
      "Epoch 56/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2306 - mae: 0.2753 - coeff_determination: 0.7678 - rmse: 1.4845 - val_loss: 0.2834 - val_mae: 0.3272 - val_coeff_determination: 0.7254 - val_rmse: 1.5918\n",
      "Epoch 57/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2280 - mae: 0.2744 - coeff_determination: 0.7709 - rmse: 1.4798 - val_loss: 0.2841 - val_mae: 0.3320 - val_coeff_determination: 0.7244 - val_rmse: 1.5930\n",
      "Epoch 58/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2306 - mae: 0.2735 - coeff_determination: 0.7683 - rmse: 1.4841 - val_loss: 0.2808 - val_mae: 0.3323 - val_coeff_determination: 0.7281 - val_rmse: 1.5864\n",
      "Epoch 59/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2278 - mae: 0.2740 - coeff_determination: 0.7710 - rmse: 1.4810 - val_loss: 0.2796 - val_mae: 0.3168 - val_coeff_determination: 0.7295 - val_rmse: 1.5794\n",
      "Epoch 60/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2296 - mae: 0.2737 - coeff_determination: 0.7697 - rmse: 1.4824 - val_loss: 0.2800 - val_mae: 0.3104 - val_coeff_determination: 0.7294 - val_rmse: 1.5795\n",
      "Epoch 61/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2271 - mae: 0.2717 - coeff_determination: 0.7713 - rmse: 1.4774 - val_loss: 0.2780 - val_mae: 0.3114 - val_coeff_determination: 0.7318 - val_rmse: 1.5774\n",
      "Epoch 62/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2263 - mae: 0.2706 - coeff_determination: 0.7724 - rmse: 1.4764 - val_loss: 0.2812 - val_mae: 0.3176 - val_coeff_determination: 0.7285 - val_rmse: 1.5842\n",
      "Epoch 63/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2253 - mae: 0.2698 - coeff_determination: 0.7739 - rmse: 1.4742 - val_loss: 0.2813 - val_mae: 0.3153 - val_coeff_determination: 0.7284 - val_rmse: 1.5844\n",
      "Epoch 64/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2253 - mae: 0.2680 - coeff_determination: 0.7731 - rmse: 1.4727 - val_loss: 0.2786 - val_mae: 0.3084 - val_coeff_determination: 0.7304 - val_rmse: 1.5750\n",
      "Epoch 65/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2266 - mae: 0.2682 - coeff_determination: 0.7721 - rmse: 1.4753 - val_loss: 0.2797 - val_mae: 0.3200 - val_coeff_determination: 0.7303 - val_rmse: 1.5860\n",
      "Epoch 66/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2274 - mae: 0.2693 - coeff_determination: 0.7712 - rmse: 1.4779 - val_loss: 0.2785 - val_mae: 0.3163 - val_coeff_determination: 0.7304 - val_rmse: 1.5776\n",
      "Epoch 67/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2251 - mae: 0.2691 - coeff_determination: 0.7737 - rmse: 1.4734 - val_loss: 0.2757 - val_mae: 0.3110 - val_coeff_determination: 0.7343 - val_rmse: 1.5719\n",
      "Epoch 68/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2243 - mae: 0.2668 - coeff_determination: 0.7736 - rmse: 1.4707 - val_loss: 0.2772 - val_mae: 0.3141 - val_coeff_determination: 0.7324 - val_rmse: 1.5759\n",
      "Epoch 69/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2235 - mae: 0.2673 - coeff_determination: 0.7751 - rmse: 1.4695 - val_loss: 0.2797 - val_mae: 0.3128 - val_coeff_determination: 0.7298 - val_rmse: 1.5808\n",
      "Epoch 70/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2233 - mae: 0.2661 - coeff_determination: 0.7761 - rmse: 1.4692 - val_loss: 0.2758 - val_mae: 0.3024 - val_coeff_determination: 0.7338 - val_rmse: 1.5673\n",
      "Epoch 71/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2231 - mae: 0.2655 - coeff_determination: 0.7763 - rmse: 1.4675 - val_loss: 0.2731 - val_mae: 0.3047 - val_coeff_determination: 0.7368 - val_rmse: 1.5635\n",
      "Epoch 72/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2211 - mae: 0.2629 - coeff_determination: 0.7771 - rmse: 1.4648 - val_loss: 0.2738 - val_mae: 0.3106 - val_coeff_determination: 0.7358 - val_rmse: 1.5682\n",
      "Epoch 73/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2236 - mae: 0.2648 - coeff_determination: 0.7741 - rmse: 1.4694 - val_loss: 0.2749 - val_mae: 0.3052 - val_coeff_determination: 0.7345 - val_rmse: 1.5686\n",
      "Epoch 74/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2223 - mae: 0.2660 - coeff_determination: 0.7766 - rmse: 1.4673 - val_loss: 0.2726 - val_mae: 0.2981 - val_coeff_determination: 0.7364 - val_rmse: 1.5582\n",
      "Epoch 75/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2224 - mae: 0.2640 - coeff_determination: 0.7773 - rmse: 1.4667 - val_loss: 0.2780 - val_mae: 0.3033 - val_coeff_determination: 0.7314 - val_rmse: 1.5729\n",
      "Epoch 76/500\n",
      "11872/11872 [==============================] - 1s 77us/step - loss: 0.2235 - mae: 0.2640 - coeff_determination: 0.7746 - rmse: 1.4684 - val_loss: 0.2761 - val_mae: 0.3064 - val_coeff_determination: 0.7330 - val_rmse: 1.5711\n",
      "Epoch 77/500\n",
      "11872/11872 [==============================] - 1s 79us/step - loss: 0.2234 - mae: 0.2638 - coeff_determination: 0.7754 - rmse: 1.4691 - val_loss: 0.2720 - val_mae: 0.3028 - val_coeff_determination: 0.7371 - val_rmse: 1.5583\n",
      "Epoch 78/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2226 - mae: 0.2632 - coeff_determination: 0.7759 - rmse: 1.4667 - val_loss: 0.2742 - val_mae: 0.3098 - val_coeff_determination: 0.7355 - val_rmse: 1.5694\n",
      "Epoch 79/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2233 - mae: 0.2640 - coeff_determination: 0.7750 - rmse: 1.4683 - val_loss: 0.2755 - val_mae: 0.3054 - val_coeff_determination: 0.7339 - val_rmse: 1.5673\n",
      "Epoch 80/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2210 - mae: 0.2626 - coeff_determination: 0.7775 - rmse: 1.4644 - val_loss: 0.2751 - val_mae: 0.3038 - val_coeff_determination: 0.7349 - val_rmse: 1.5683\n",
      "Epoch 81/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2204 - mae: 0.2621 - coeff_determination: 0.7782 - rmse: 1.4630 - val_loss: 0.2811 - val_mae: 0.2977 - val_coeff_determination: 0.7286 - val_rmse: 1.5767\n",
      "Epoch 82/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2219 - mae: 0.2632 - coeff_determination: 0.7762 - rmse: 1.4664 - val_loss: 0.2722 - val_mae: 0.2953 - val_coeff_determination: 0.7370 - val_rmse: 1.5571\n",
      "Epoch 83/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2212 - mae: 0.2623 - coeff_determination: 0.7766 - rmse: 1.4639 - val_loss: 0.2738 - val_mae: 0.2934 - val_coeff_determination: 0.7357 - val_rmse: 1.5578\n",
      "Epoch 84/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2193 - mae: 0.2589 - coeff_determination: 0.7793 - rmse: 1.4601 - val_loss: 0.2751 - val_mae: 0.3014 - val_coeff_determination: 0.7346 - val_rmse: 1.5676\n",
      "Epoch 85/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2195 - mae: 0.2605 - coeff_determination: 0.7792 - rmse: 1.4607 - val_loss: 0.2743 - val_mae: 0.3011 - val_coeff_determination: 0.7346 - val_rmse: 1.5629\n",
      "Epoch 86/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2206 - mae: 0.2620 - coeff_determination: 0.7778 - rmse: 1.4624 - val_loss: 0.2767 - val_mae: 0.3172 - val_coeff_determination: 0.7322 - val_rmse: 1.5730\n",
      "Epoch 87/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2199 - mae: 0.2619 - coeff_determination: 0.7793 - rmse: 1.4619 - val_loss: 0.2735 - val_mae: 0.2927 - val_coeff_determination: 0.7357 - val_rmse: 1.5579\n",
      "Epoch 88/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2187 - mae: 0.2593 - coeff_determination: 0.7801 - rmse: 1.4593 - val_loss: 0.2750 - val_mae: 0.3033 - val_coeff_determination: 0.7347 - val_rmse: 1.5690\n",
      "Epoch 89/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2189 - mae: 0.2593 - coeff_determination: 0.7800 - rmse: 1.4593 - val_loss: 0.2743 - val_mae: 0.2970 - val_coeff_determination: 0.7348 - val_rmse: 1.5625\n",
      "Epoch 90/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2209 - mae: 0.2605 - coeff_determination: 0.7777 - rmse: 1.4628 - val_loss: 0.2732 - val_mae: 0.2985 - val_coeff_determination: 0.7364 - val_rmse: 1.5622\n",
      "Epoch 91/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2202 - mae: 0.2610 - coeff_determination: 0.7779 - rmse: 1.4614 - val_loss: 0.2725 - val_mae: 0.2986 - val_coeff_determination: 0.7361 - val_rmse: 1.5597\n",
      "Epoch 92/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2189 - mae: 0.2596 - coeff_determination: 0.7803 - rmse: 1.4592 - val_loss: 0.2756 - val_mae: 0.2996 - val_coeff_determination: 0.7330 - val_rmse: 1.5631\n",
      "Epoch 93/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2208 - mae: 0.2599 - coeff_determination: 0.7784 - rmse: 1.4612 - val_loss: 0.2755 - val_mae: 0.2970 - val_coeff_determination: 0.7334 - val_rmse: 1.5654\n",
      "Epoch 94/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2210 - mae: 0.2615 - coeff_determination: 0.7774 - rmse: 1.4628 - val_loss: 0.2740 - val_mae: 0.2921 - val_coeff_determination: 0.7352 - val_rmse: 1.5602\n",
      "Epoch 95/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2187 - mae: 0.2587 - coeff_determination: 0.7803 - rmse: 1.4592 - val_loss: 0.2720 - val_mae: 0.3015 - val_coeff_determination: 0.7374 - val_rmse: 1.5605\n",
      "Epoch 96/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2185 - mae: 0.2582 - coeff_determination: 0.7801 - rmse: 1.4583 - val_loss: 0.2760 - val_mae: 0.3002 - val_coeff_determination: 0.7323 - val_rmse: 1.5638\n",
      "Epoch 97/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2208 - mae: 0.2603 - coeff_determination: 0.7777 - rmse: 1.4617 - val_loss: 0.2717 - val_mae: 0.2987 - val_coeff_determination: 0.7369 - val_rmse: 1.5561\n",
      "Epoch 98/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2194 - mae: 0.2597 - coeff_determination: 0.7803 - rmse: 1.4596 - val_loss: 0.2751 - val_mae: 0.2911 - val_coeff_determination: 0.7335 - val_rmse: 1.5603\n",
      "Epoch 99/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2181 - mae: 0.2576 - coeff_determination: 0.7808 - rmse: 1.4573 - val_loss: 0.2717 - val_mae: 0.2979 - val_coeff_determination: 0.7373 - val_rmse: 1.5587\n",
      "Epoch 100/500\n",
      "11872/11872 [==============================] - 1s 77us/step - loss: 0.2188 - mae: 0.2575 - coeff_determination: 0.7802 - rmse: 1.4585 - val_loss: 0.2757 - val_mae: 0.3088 - val_coeff_determination: 0.7336 - val_rmse: 1.5724\n",
      "Epoch 101/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2177 - mae: 0.2589 - coeff_determination: 0.7807 - rmse: 1.4574 - val_loss: 0.2735 - val_mae: 0.3030 - val_coeff_determination: 0.7357 - val_rmse: 1.5612\n",
      "Epoch 102/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2206 - mae: 0.2615 - coeff_determination: 0.7777 - rmse: 1.4616 - val_loss: 0.2738 - val_mae: 0.2847 - val_coeff_determination: 0.7357 - val_rmse: 1.5567\n",
      "Epoch 103/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2175 - mae: 0.2567 - coeff_determination: 0.7811 - rmse: 1.4559 - val_loss: 0.2741 - val_mae: 0.2935 - val_coeff_determination: 0.7353 - val_rmse: 1.5595\n",
      "Epoch 104/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2183 - mae: 0.2577 - coeff_determination: 0.7806 - rmse: 1.4574 - val_loss: 0.2752 - val_mae: 0.2891 - val_coeff_determination: 0.7335 - val_rmse: 1.5581\n",
      "Epoch 105/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2175 - mae: 0.2565 - coeff_determination: 0.7813 - rmse: 1.4559 - val_loss: 0.2725 - val_mae: 0.2925 - val_coeff_determination: 0.7370 - val_rmse: 1.5554\n",
      "Epoch 106/500\n",
      "11872/11872 [==============================] - 1s 77us/step - loss: 0.2183 - mae: 0.2577 - coeff_determination: 0.7806 - rmse: 1.4571 - val_loss: 0.2731 - val_mae: 0.2968 - val_coeff_determination: 0.7363 - val_rmse: 1.5595\n",
      "Epoch 107/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2165 - mae: 0.2575 - coeff_determination: 0.7822 - rmse: 1.4557 - val_loss: 0.2744 - val_mae: 0.2949 - val_coeff_determination: 0.7344 - val_rmse: 1.5598\n",
      "Epoch 108/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2178 - mae: 0.2565 - coeff_determination: 0.7807 - rmse: 1.4562 - val_loss: 0.2773 - val_mae: 0.3032 - val_coeff_determination: 0.7314 - val_rmse: 1.5708\n",
      "Epoch 109/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2169 - mae: 0.2569 - coeff_determination: 0.7823 - rmse: 1.4551 - val_loss: 0.2708 - val_mae: 0.2937 - val_coeff_determination: 0.7389 - val_rmse: 1.5518\n",
      "Epoch 110/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2173 - mae: 0.2578 - coeff_determination: 0.7812 - rmse: 1.4556 - val_loss: 0.2763 - val_mae: 0.3033 - val_coeff_determination: 0.7330 - val_rmse: 1.5675\n",
      "Epoch 111/500\n",
      "11872/11872 [==============================] - 1s 77us/step - loss: 0.2159 - mae: 0.2552 - coeff_determination: 0.7830 - rmse: 1.4530 - val_loss: 0.2738 - val_mae: 0.2983 - val_coeff_determination: 0.7348 - val_rmse: 1.5585\n",
      "Epoch 112/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2180 - mae: 0.2569 - coeff_determination: 0.7814 - rmse: 1.4566 - val_loss: 0.2726 - val_mae: 0.3003 - val_coeff_determination: 0.7365 - val_rmse: 1.5574\n",
      "Epoch 113/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2173 - mae: 0.2572 - coeff_determination: 0.7819 - rmse: 1.4546 - val_loss: 0.2744 - val_mae: 0.2886 - val_coeff_determination: 0.7349 - val_rmse: 1.5578\n",
      "Epoch 114/500\n",
      "11872/11872 [==============================] - 1s 75us/step - loss: 0.2168 - mae: 0.2568 - coeff_determination: 0.7819 - rmse: 1.4554 - val_loss: 0.2899 - val_mae: 0.3067 - val_coeff_determination: 0.7177 - val_rmse: 1.5828\n",
      "Epoch 115/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2180 - mae: 0.2568 - coeff_determination: 0.7810 - rmse: 1.4570 - val_loss: 0.2758 - val_mae: 0.2959 - val_coeff_determination: 0.7330 - val_rmse: 1.5618\n",
      "Epoch 116/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2167 - mae: 0.2559 - coeff_determination: 0.7827 - rmse: 1.4551 - val_loss: 0.2731 - val_mae: 0.3005 - val_coeff_determination: 0.7355 - val_rmse: 1.5592\n",
      "Epoch 117/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2166 - mae: 0.2559 - coeff_determination: 0.7823 - rmse: 1.4535 - val_loss: 0.2787 - val_mae: 0.3000 - val_coeff_determination: 0.7291 - val_rmse: 1.5735\n",
      "Epoch 118/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2170 - mae: 0.2560 - coeff_determination: 0.7822 - rmse: 1.4546 - val_loss: 0.2720 - val_mae: 0.3001 - val_coeff_determination: 0.7370 - val_rmse: 1.5586\n",
      "Epoch 119/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2172 - mae: 0.2568 - coeff_determination: 0.7821 - rmse: 1.4553 - val_loss: 0.2753 - val_mae: 0.2939 - val_coeff_determination: 0.7336 - val_rmse: 1.5617\n",
      "Epoch 120/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2179 - mae: 0.2566 - coeff_determination: 0.7808 - rmse: 1.4565 - val_loss: 0.2728 - val_mae: 0.2975 - val_coeff_determination: 0.7355 - val_rmse: 1.5567\n",
      "Epoch 121/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2167 - mae: 0.2564 - coeff_determination: 0.7823 - rmse: 1.4544 - val_loss: 0.2718 - val_mae: 0.2966 - val_coeff_determination: 0.7363 - val_rmse: 1.5551\n",
      "Epoch 122/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2156 - mae: 0.2552 - coeff_determination: 0.7833 - rmse: 1.4516 - val_loss: 0.2741 - val_mae: 0.2961 - val_coeff_determination: 0.7341 - val_rmse: 1.5607\n",
      "Epoch 123/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2158 - mae: 0.2548 - coeff_determination: 0.7827 - rmse: 1.4527 - val_loss: 0.2743 - val_mae: 0.2955 - val_coeff_determination: 0.7341 - val_rmse: 1.5603\n",
      "Epoch 124/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2160 - mae: 0.2548 - coeff_determination: 0.7825 - rmse: 1.4527 - val_loss: 0.2740 - val_mae: 0.2934 - val_coeff_determination: 0.7336 - val_rmse: 1.5600\n",
      "Epoch 125/500\n",
      "11872/11872 [==============================] - 1s 73us/step - loss: 0.2154 - mae: 0.2553 - coeff_determination: 0.7830 - rmse: 1.4522 - val_loss: 0.2758 - val_mae: 0.2903 - val_coeff_determination: 0.7322 - val_rmse: 1.5621\n",
      "Epoch 126/500\n",
      "11872/11872 [==============================] - 1s 76us/step - loss: 0.2159 - mae: 0.2560 - coeff_determination: 0.7832 - rmse: 1.4537 - val_loss: 0.2742 - val_mae: 0.2914 - val_coeff_determination: 0.7340 - val_rmse: 1.5580\n",
      "Epoch 127/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2155 - mae: 0.2552 - coeff_determination: 0.7836 - rmse: 1.4517 - val_loss: 0.2738 - val_mae: 0.2877 - val_coeff_determination: 0.7345 - val_rmse: 1.5574\n",
      "Epoch 128/500\n",
      "11872/11872 [==============================] - 1s 74us/step - loss: 0.2147 - mae: 0.2533 - coeff_determination: 0.7835 - rmse: 1.4507 - val_loss: 0.2744 - val_mae: 0.3003 - val_coeff_determination: 0.7340 - val_rmse: 1.5624\n",
      "Epoch 129/500\n",
      "11872/11872 [==============================] - 1s 77us/step - loss: 0.2151 - mae: 0.2553 - coeff_determination: 0.7837 - rmse: 1.4514 - val_loss: 0.2729 - val_mae: 0.2872 - val_coeff_determination: 0.7351 - val_rmse: 1.5519\n",
      "after train, finish time: 1585600189.329\n",
      "training time {} 116.288\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 260)               9100      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 260)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               78300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 240)               72240     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 241       \n",
      "=================================================================\n",
      "Total params: 159,881\n",
      "Trainable params: 159,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimizer=Adam(learning_rate=0.00015)\n",
    "model, history, time_training = build_train_ann(\n",
    "            x_train,\n",
    "            y_train_log_std,\n",
    "            x_val,\n",
    "            y_val_log_std,\n",
    "            260,\n",
    "            300,\n",
    "            240,\n",
    "            500,\n",
    "            optimizer,\n",
    "            0.25,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Data 3221.6339280934617\n",
      "R2 SCORE Data 0.009449106198590074\n",
      "RMSE Val 3695.381437452946\n",
      "R2 SCORE Val -0.02544755684684774\n",
      "RMSE Test 2133.756315303828\n",
      "R2 SCORE Test 0.02470239740147584\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_metrics(model, scalery, x_train, y_train, label_set=\"Data\")\n",
    "get_metrics(model, scalery, x_val, y_val, label_set=\"Val\")\n",
    "get_metrics(model, scalery, x_test, y_test, label_set=\"Test\")\n",
    "plot_history(history,['loss','val_loss',])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
